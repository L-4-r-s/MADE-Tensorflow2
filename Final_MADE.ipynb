{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L-4-r-s/MADE-Tensorflow2/blob/main/Final_MADE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXQf2An5H4WY"
      },
      "source": [
        "Mandatory imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "caP1QyLTHjm1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from keras import backend as k\n",
        "from keras import metrics\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Input, Layer\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIyry2dH9XA"
      },
      "source": [
        "Load Dataset ( https://github.com/mgermain/MADE/releases/download/ICML2015/binarized_mnist.npz )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLAPiPxjfeXG"
      },
      "outputs": [],
      "source": [
        "# Load Mnist Data used for FedWeIT experiments\n",
        "\n",
        "def _generate_mnist():\n",
        "    global X_train, X_valid, X_test \n",
        "    seprate_ratio = (5/7, 1/7, 1/7)\n",
        "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "    x_temp = np.concatenate((X_train, X_test))\n",
        "    x = x_temp.reshape(x_temp.shape[0],x_temp.shape[1]*x_temp.shape[2]) #flatten 28x28 pixels to one dimension (784 inputs)\n",
        "    x = np.where(x > 127, 1, 0) #binarize x\n",
        "    y = np.concatenate((Y_train, Y_test))\n",
        "    \n",
        "    idx_shuffled = np.arange(len(x))\n",
        "    random.seed(777)\n",
        "    random.shuffle(idx_shuffled)\n",
        "    labels = [5,8,7]\n",
        "    \n",
        "    idx = np.concatenate([np.where(y[:]==c)[0] for c in labels], axis=0)\n",
        "    #shuffle so samples are not sampled by class cause of previous concat operation\n",
        "    random.seed(777)\n",
        "    random.shuffle(idx) # shuffle order of training samples derived for current task\n",
        "    x_task = x[idx]\n",
        "\n",
        "    num_examples = len(x_task)\n",
        "    num_train = int(num_examples*seprate_ratio[0])\n",
        "    num_test = int(num_examples*seprate_ratio[1])\n",
        "    num_valid = num_examples - num_train - num_test\n",
        "    X_train = x_task[:num_train]\n",
        "    X_valid = x_task[num_train+num_test:]\n",
        "    X_test = x_task[num_train:num_train+num_test]\n",
        "\n",
        "_generate_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HSWUH1VI6Nc",
        "outputId": "e782b1a2-fc76-48c6-b8b0-214406b7c9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset shape: (50000, 784), train dataset shape: (10000, 784), train dataset shape = (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# I stored the dataset in my drive\n",
        "data_path = 'drive/MyDrive/binarized_mnist.npz'\n",
        "binarized_mnist = np.load(data_path)\n",
        "X_train, X_valid, X_test = binarized_mnist['train_data'], binarized_mnist['valid_data'], binarized_mnist['test_data']\n",
        "print(f\"train dataset shape: {X_train.shape}, train dataset shape: {X_test.shape}, train dataset shape = {X_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOpK6CayJgnf"
      },
      "source": [
        "Create Mask Generator Module for creating/managing MADEs masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rJJWPHU4Jnt-"
      },
      "outputs": [],
      "source": [
        "class MaskGenerator(object):\n",
        "  # num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "  # units_per_layer = Array containing # of units per layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "  # natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "  # current_mask: Integer to keep track of the mask currently used (xth mask)\n",
        "  # m: The mask values assigned to the networks units. 0 is the index of the input layer, 1 is the index of the first hidden layer and so on\n",
        "  def __init__(self, num_masks, units_per_layer, natural_input_order = False, seed=42):\n",
        "    self.num_masks = num_masks\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.seed = seed\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.current_mask = 0\n",
        "    self.m = {}\n",
        "\n",
        "    if natural_input_order: # init input ordering according to settings\n",
        "      self.m[0] = np.arange(self.units_per_layer[0])\n",
        "    else:\n",
        "      self.shuffle_inputs(return_mask = False)\n",
        "  \n",
        "  #Iterate through the hidden layers, resample new connectivity values m and build/return the resulting new masks\n",
        "  def shuffle_masks(self):\n",
        "    layer_amount = len(self.units_per_layer)\n",
        "    rng = np.random.RandomState(self.seed+self.current_mask)\n",
        "    self.current_mask = (self.current_mask + 1) % self.num_masks # Cycle through masks\n",
        "    for i in range(1, layer_amount -1): #skip input layer & output layer and only iterate through hidden_layers\n",
        "      self.m[i] = rng.randint(self.m[i-1].min(), self.units_per_layer[0] -1, size = self.units_per_layer[i]) # sample m from [min_m(previous_layer, d-1)] for all hidden units\n",
        "    new_masks = [tf.convert_to_tensor((self.m[l-1][:, None] <= self.m[l][None,:]), dtype=np.float32) for l in range(1, layer_amount-1)] # build hidden layer masks\n",
        "    new_masks.append(tf.convert_to_tensor((self.m[layer_amount-2][:, None] < self.m[0][None, :]), dtype = np.float32)) #build output layer mask. Note that the m values for the output layer are the same as for the input layer \n",
        "    return new_masks\n",
        "\n",
        "  # builds & returns direct mask. Call this method after shuffling inputs if order_agnostic training is active.\n",
        "  # Note that the Mask values m are the same for both input and output layers\n",
        "  def get_direct_mask(self):\n",
        "    return tf.convert_to_tensor((self.m[0][:, None] < self.m[0][None, :]), dtype = np.float32)\n",
        "\n",
        "  # shuffle input ordering and return new mask for first hidden layer\n",
        "  def shuffle_inputs(self, return_mask = True):\n",
        "    self.m[0] = np.random.permutation(self.units_per_layer[0])\n",
        "    if return_mask:\n",
        "      return tf.convert_to_tensor((self.m[0][:, None] <= self.m[1][None,:]), dtype=np.float32)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGbtslRZYoV2"
      },
      "source": [
        "Custom Layer for MADE masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "25TXVUagYnUI"
      },
      "outputs": [],
      "source": [
        "# should be self explaining\n",
        "class MaskedLayer(Layer):\n",
        "    def __init__(self,\n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                **kwargs):\n",
        "      self.units = units\n",
        "      self.mask = mask\n",
        "      self.activation = activations.get(activation)\n",
        "      self.kernel_initializer = kernel_initializer\n",
        "      self.bias_initializer = bias_initializer\n",
        "      super(MaskedLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      #self.input_dim = input_shape[-1] if self.x_dim is None else input_shape[0][-1]\n",
        "\n",
        "      self.W = self.add_weight(shape=self.mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='W')\n",
        "\n",
        "      self.bias = self.add_weight(shape=(self.units,),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      name='bias')\n",
        "\n",
        "      self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        ## Modified keras.Dense to account for the mask\n",
        "        masked_weights = self.W*self.mask\n",
        "        output = k.dot(inputs, masked_weights)\n",
        "        output = k.bias_add(output, self.bias, data_format = 'channels_last')\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def set_mask(self, mask):\n",
        "        self.mask = mask\n",
        "\n",
        "    def get_mask(self):\n",
        "        return self.mask\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ##Same as keras.Dense\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        assert input_shape[-1]\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionningMaskedLayer(MaskedLayer):\n",
        "    def __init__(self, \n",
        "                units,\n",
        "                mask,\n",
        "                activation='relu',\n",
        "                kernel_initializer='glorot_uniform',\n",
        "                bias_initializer='zeros',\n",
        "                use_cond_mask=False,\n",
        "                **kwargs):\n",
        "        self.use_cond_mask = use_cond_mask\n",
        "        super(ConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.use_cond_mask:\n",
        "            self.U = self.add_weight(shape=self.mask.shape,\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='U')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.use_cond_mask == False:\n",
        "          return super().call(inputs)\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(inputs)),self.U*self.mask)\n",
        "        weighted_input = k.dot(inputs, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        output = weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class DirectInputConnectConditionningMaskedLayer(ConditionningMaskedLayer):\n",
        "      def __init__(self,\n",
        "                   units,\n",
        "                   mask,\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='glorot_uniform',\n",
        "                   bias_initializer='zeros',\n",
        "                   use_cond_mask=False,\n",
        "                   direct_mask = None,\n",
        "                **kwargs):\n",
        "        self.direct_mask = direct_mask\n",
        "        super(DirectInputConnectConditionningMaskedLayer, self).__init__(units,\n",
        "                mask,\n",
        "                activation,\n",
        "                kernel_initializer,\n",
        "                bias_initializer,\n",
        "                use_cond_mask,\n",
        "                **kwargs)\n",
        "\n",
        "      def build(self, input_shape):\n",
        "        if self.direct_mask is not None:\n",
        "          self.D = self.add_weight(shape=self.direct_mask.shape,\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  name='D')\n",
        "        super().build(input_shape)\n",
        "\n",
        "      def set_mask(self, mask, direct = False):\n",
        "        if direct:\n",
        "          self.direct_mask = mask\n",
        "        else:\n",
        "          super().set_mask(mask)\n",
        "\n",
        "      def get_mask(self, direct = False):\n",
        "        if direct:\n",
        "          return self.direct_mask\n",
        "        else:\n",
        "          return super().get_mask\n",
        "\n",
        "      def call(self, inputs):\n",
        "        if self.direct_mask is None:\n",
        "          return super().call(inputs)\n",
        "        input, direct_input = inputs[0], inputs[1]\n",
        "\n",
        "        masked_w_weights = self.W*self.mask\n",
        "        weighted_input = k.dot(input, masked_w_weights)\n",
        "        weighted_input_and_bias = k.bias_add(weighted_input, self.bias, data_format = 'channels_last')\n",
        "        weighted_direct_input = k.dot(direct_input, self.D * self.direct_mask)\n",
        "\n",
        "        if self.use_cond_mask:\n",
        "          masked_u_weights_times_one_vec = k.dot(tf.ones(tf.shape(input)),self.U*self.mask)\n",
        "          output = weighted_direct_input + weighted_input_and_bias + masked_u_weights_times_one_vec\n",
        "\n",
        "        else: output = weighted_direct_input + weighted_input_and_bias\n",
        "\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGI80LjceZ3x"
      },
      "source": [
        "# MADE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lgjFxzNDeXQg"
      },
      "outputs": [],
      "source": [
        "# outputs: output Layer   ---------- Both needed when using ----------\n",
        "# inputs: input Layer     ----------    base keras.Model    ----------     \n",
        "# mask_generator: Mask Generator instance that manages the Models Masks\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# conn_agn: Boolean defining if training should be connectivity_agnostic\n",
        "# direct_input: Boolean defining if direct input masks should be used\n",
        "class ModelMADE(tf.keras.Model):\n",
        "    def __init__(self, inputs, outputs, mask_generator, order_agn, conn_agn,\n",
        "                 #order_agn_step_size, conn_agn_step_size, \n",
        "                 direct_input, **kwargs):\n",
        "      super(ModelMADE, self).__init__(inputs = inputs, outputs = outputs, **kwargs)\n",
        "      self.mask_generator = mask_generator\n",
        "      self.order_agn = order_agn\n",
        "      self.conn_agn = conn_agn\n",
        "     # self.order_agn_step_size = order_agn_step_size\n",
        "     # self.conn_agn_step_size = conn_agn_step_size\n",
        "      self.direct_input = direct_input\n",
        "     # self.step = 1\n",
        "    \n",
        "    # Method called by fit for every batch\n",
        "    def train_step(self, data):\n",
        "\n",
        "\n",
        "      # reoder inputs, change masks\n",
        "      if self.order_agn:\n",
        "        # order agnostic and connectivity agnostic training\n",
        "        if self.conn_agn:\n",
        "          self.mask_generator.shuffle_inputs(return_mask = False)\n",
        "          new_masks = self.mask_generator.shuffle_masks()\n",
        "          for hidden_layer_id in range(len(new_masks)):\n",
        "            self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id]) #assign layer+1 since the first layer is no hidden layer and has no mask\n",
        "        \n",
        "        # order agnostic but not connectivity agnostic training        \n",
        "        else:\n",
        "          self.layers[1].set_mask(self.mask_generator.shuffle_inputs())\n",
        "        if self.direct_input:\n",
        "          self.layers[-1].set_mask(self.mask_generator.get_direct_mask(), direct=True)\n",
        "\n",
        "      # not order agnostic but connectivity agnostic training\n",
        "      elif self.conn_agn:\n",
        "        new_masks = self.mask_generator.shuffle_masks()\n",
        "        for hidden_layer_id in range(len(new_masks)):\n",
        "          self.layers[1+hidden_layer_id].set_mask(new_masks[hidden_layer_id])\n",
        "\n",
        "\n",
        "      # Unpack the data. Its structure depends on your model and\n",
        "      # on what you pass to `fit()`.\n",
        "      x, y = data\n",
        "\n",
        "    #  self.step += 1\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)  # Forward pass\n",
        "        # Compute the loss value\n",
        "        # (the loss function is configured in `compile()`)\n",
        "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "      \n",
        "\n",
        "      # Compute gradients\n",
        "      trainable_vars = self.trainable_variables\n",
        "      gradients = tape.gradient(loss, trainable_vars)\n",
        "      # Update weights\n",
        "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "      # Update metrics (includes the metric that tracks the loss)\n",
        "      self.compiled_metrics.update_state(y, y_pred)\n",
        "      # Return a dict mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bz-62ojlwZ"
      },
      "source": [
        "# MADE Object\n",
        "responsible for building and inintalizing the MADE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gVyYE-CijlSo"
      },
      "outputs": [],
      "source": [
        "# units_per_layer = Array containing # of units per layer\n",
        "# natural_input_order = Boolean defining if the natural input order (x1, x2, x3 etc) should be used\n",
        "# num_masks: The amount of masks that will be cycled through during training. if num_masks == 1 then connectivity agnostic training is disabled\n",
        "# order_agn: Boolean defining if training should be order_agnostic\n",
        "# connectivity_weights: Boolean defining if connectivity weights should be used\n",
        "# direct input: Boolean defining if there should be a direct input connection between input & output layer\n",
        "  # seed = The seed used for randomly sampling the masks, for guaranteeing reproducability\n",
        "class MADE(object):\n",
        "  def __init__(self, units_per_layer, natural_input_order, num_masks, order_agn,\n",
        "               #order_agn_step_size, conn_agn_step_size, \n",
        "               connectivity_weights, direct_input, seed = \"42\"):\n",
        "    self.units_per_layer = units_per_layer\n",
        "    self.natural_input_order = natural_input_order\n",
        "    self.num_masks = num_masks\n",
        "    self.order_agn = order_agn\n",
        "    #self.order_agn_step_size = order_agn_step_size\n",
        "    #self.conn_agn_step_size = conn_agn_step_size\n",
        "    self.connectivity_weights = connectivity_weights\n",
        "    self.direct_input = direct_input\n",
        "    self.seed = seed\n",
        "    self.mask_generator = MaskGenerator(num_masks, units_per_layer, natural_input_order, seed)\n",
        "\n",
        "  def build_model(self):\n",
        "    # build input layer\n",
        "    a = Input(shape = (self.units_per_layer[0],))\n",
        "    x_layers = []\n",
        "      \n",
        "    #build masks\n",
        "    masks = self.mask_generator.shuffle_masks()\n",
        "    direct_mask = None\n",
        "\n",
        "    #build hidden layers  \n",
        "    for i in range(1,len(self.units_per_layer)-1): #exclude input & output layer\n",
        "      if i == 1:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(a)) #activation is relu, call custom_masking with previous layer as input-param\n",
        "      else:\n",
        "        x_layers.append(ConditionningMaskedLayer(units = self.units_per_layer[i], mask = masks[i-1], use_cond_mask = self.connectivity_weights)(x_layers[i-1]))\n",
        "          \n",
        "    #build output layer, output layer's activation is sigmoid.\n",
        "    if self.direct_input:\n",
        "      direct_mask = self.mask_generator.get_direct_mask()\n",
        "      output_layer = DirectInputConnectConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights, direct_mask = direct_mask)([x_layers[-1], a])\n",
        "    else:\n",
        "      output_layer = ConditionningMaskedLayer(units = self.units_per_layer[-1], mask = masks[-1], activation='sigmoid', use_cond_mask = self.connectivity_weights)(x_layers[-1])\n",
        "    x_layers.append(output_layer)\n",
        "    \n",
        "    #self.model = Model(inputs = a, outputs = x_layers[-1])\n",
        "    self.model = ModelMADE(inputs = a, outputs = x_layers[-1], mask_generator = self.mask_generator, order_agn = self.order_agn, conn_agn = self.num_masks>1, \n",
        "                           #order_agn_step_size = self.order_agn_step_size, conn_agn_step_size = self.conn_agn_step_size, connectivity_weights=self.connectivity_weights, \n",
        "                           direct_input=self.direct_input)\n",
        "    return self.model\n",
        "\n",
        "  def summary(self):\n",
        "    return self.model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q2-yQaQpsST"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-2g0o7Ltpus7"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(x, x_decoded_mean):\n",
        "    x = k.flatten(x)\n",
        "    x_decoded_mean = k.flatten(x_decoded_mean)\n",
        "    xent_loss = len(X_train[1]) * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "    return xent_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QHrJs4TqTYq"
      },
      "source": [
        "# Build & Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icwEjgn0qRSf",
        "outputId": "cb35a0c8-afc1-449b-a606-9e1e7011d2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_made\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " conditionning_masked_layer   (None, 500)              392500    \n",
            " (ConditionningMaskedLayer)                                      \n",
            "                                                                 \n",
            " conditionning_masked_layer_  (None, 784)              392784    \n",
            " 1 (ConditionningMaskedLayer                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 785,284\n",
            "Trainable params: 785,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "500/500 [==============================] - 13s 19ms/step - loss: 183.7745 - val_loss: 137.3862\n",
            "Epoch 2/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 125.3147 - val_loss: 114.2998\n",
            "Epoch 3/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 113.5064 - val_loss: 106.7710\n",
            "Epoch 4/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 107.7093 - val_loss: 105.5125\n",
            "Epoch 5/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 104.2742 - val_loss: 98.9023\n",
            "Epoch 6/100\n",
            "500/500 [==============================] - 8s 15ms/step - loss: 101.7035 - val_loss: 100.6040\n",
            "Epoch 7/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 100.0638 - val_loss: 98.3388\n",
            "Epoch 8/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 98.5137 - val_loss: 93.6797\n",
            "Epoch 9/100\n",
            "500/500 [==============================] - 8s 17ms/step - loss: 96.9976 - val_loss: 97.5710\n",
            "Epoch 10/100\n",
            "500/500 [==============================] - 8s 17ms/step - loss: 95.9230 - val_loss: 92.1824\n",
            "Epoch 11/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 95.0755 - val_loss: 96.3960\n",
            "Epoch 12/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 94.4927 - val_loss: 95.9622\n",
            "Epoch 13/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 93.7886 - val_loss: 96.8734\n",
            "Epoch 14/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 92.8605 - val_loss: 90.0294\n",
            "Epoch 15/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 92.5725 - val_loss: 92.0300\n",
            "Epoch 16/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 91.9578 - val_loss: 91.3073\n",
            "Epoch 17/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 91.4642 - val_loss: 89.8511\n",
            "Epoch 18/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 90.8837 - val_loss: 88.5128\n",
            "Epoch 19/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 90.6177 - val_loss: 93.6599\n",
            "Epoch 20/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 89.8932 - val_loss: 88.3533\n",
            "Epoch 21/100\n",
            "500/500 [==============================] - 7s 15ms/step - loss: 89.8234 - val_loss: 90.2480\n",
            "Epoch 22/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 89.7096 - val_loss: 93.4472\n",
            "Epoch 23/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 89.0241 - val_loss: 84.2765\n",
            "Epoch 24/100\n",
            "500/500 [==============================] - 8s 16ms/step - loss: 89.1283 - val_loss: 86.4623\n",
            "Epoch 25/100\n",
            " 13/500 [..............................] - ETA: 6s - loss: 89.6068"
          ]
        }
      ],
      "source": [
        "######################### Settings #########################\n",
        "_optimizer_type = \"ada\" #for any other string here Adagrad is used\n",
        "_adam_lr = 0.001 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_lr = 0.01 #0.1, 0.05, 0.01, 0.005\n",
        "_ada_epsilon = 1e-6\n",
        "\n",
        "_hidden_layers = [500]\n",
        "_natural_input_order = False,\n",
        "_num_masks = 1\n",
        "_order_agn = True\n",
        "_order_agn_step_size = 1\n",
        "_conn_agn_step_size = 1\n",
        "_connectivity_weights = False\n",
        "_direct_input = False\n",
        "_seed = 42\n",
        "_batch_size = 100\n",
        "_epochs = 100\n",
        "_data_portion = 1\n",
        "\n",
        "if _optimizer_type == \"adam\": optimizer = Adam(_adam_lr)\n",
        "else: optimizer = Adagrad(_ada_lr, epsilon = _ada_epsilon)\n",
        "\n",
        "test_losses = 0\n",
        "array = []\n",
        "train_size = int(len(X_train)* _data_portion)\n",
        "for i in range(10):\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  if _data_portion != 1:\n",
        "    print(type(train_size))\n",
        "    idx_shuffled = np.arange(len(X_train))\n",
        "    random.seed(_seed+i)\n",
        "    random.shuffle(idx_shuffled)\n",
        "    X_portion = X_train[idx_shuffled]\n",
        "    X_portion = X_train[:train_size]\n",
        "    print(len(X_train))\n",
        "\n",
        "  else:\n",
        "    X_portion = X_train\n",
        "\n",
        "  units_per_layer = np.concatenate(([len(X_train[0])], _hidden_layers, [len(X_train[0])])) #in MADE case the input & output layer have the same amount of units\n",
        "\n",
        "  temp = MADE(units_per_layer, natural_input_order=_natural_input_order, num_masks = _num_masks, order_agn = _order_agn, \n",
        "              #order_agn_step_size = _order_agn_step_size, conn_agn_step_size = _conn_agn_step_size, \n",
        "              connectivity_weights = _connectivity_weights, direct_input = _direct_input, seed = _seed)\n",
        "  model = temp.build_model()\n",
        "  model.compile(optimizer=optimizer, loss=cross_entropy_loss, run_eagerly=True)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  plt.history = model.fit(\n",
        "      X_portion, X_portion,\n",
        "      batch_size=_batch_size,\n",
        "      epochs=_epochs,\n",
        "      validation_data=(X_valid, X_valid),\n",
        "  )\n",
        "  done = time.time()\n",
        "  elapsed = done - start\n",
        "  print(\"Elapsed: \", elapsed)\n",
        "  print(f\"Number of masks: {_num_masks}\")\n",
        "  test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)\n",
        "  print(f\"Test Loss: {test_loss}\")\n",
        "  test_losses+=test_loss\n",
        "  array.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYEqvLlvI6oW",
        "outputId": "18e73d8a-5ebe-43a6-b61e-0e7ffdde2763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 1s 7ms/step - loss: 92.9661\n",
            "Test Loss: 92.96611022949219\n"
          ]
        }
      ],
      "source": [
        "  test_loss=model.evaluate(X_test, X_test, batch_size=_batch_size)\n",
        "  print(f\"Test Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGXjBqV7d1ck",
        "outputId": "066c151a-4f3c-49c5-8c12-8ee85c13cbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101.92285919189453, 101.9369125366211, 102.05867767333984, 102.13648986816406, 101.84507751464844, 101.95286560058594, 101.79920959472656, 101.95958709716797, 101.90636444091797, 101.86487579345703]\n",
            "101.93829193115235\n",
            "0.09440509251146503\n",
            "101.93829193115235\n"
          ]
        }
      ],
      "source": [
        "print(array)\n",
        "print(test_losses/10)\n",
        "print(np.std(array))\n",
        "print(np.mean(array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lobz3VbIwSsQ"
      },
      "outputs": [],
      "source": [
        "0.1401634688050654\n",
        "100.89565582275391\n",
        "1 mask \n",
        "\n",
        "0.09004508236788293\n",
        "98.4830337524414\n",
        "2 masks\n",
        "\n",
        "0.07379556570556475\n",
        "99.12493896484375\n",
        "3 masks\n",
        "\n",
        "0.06985505011918437\n",
        "99.83944549560547\n",
        "5 masks\n",
        "\n",
        "0.09187954082102019\n",
        "100.64412994384766\n",
        "10 masks\n",
        "\n",
        "0.09440509251146503\n",
        "101.93829193115235\n",
        "100 masks"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final MADE.ipynb",
      "provenance": [],
      "mount_file_id": "1j2EcYjtSn8r_xvQF84Ach613KZpqGIZE",
      "authorship_tag": "ABX9TyONWXsXDAhvN8nmTX2t45XL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}